{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deb86824-b503-4e03-b3e1-b4fe77d40681",
   "metadata": {},
   "source": [
    "# Part 2 NLP (25 pts)\n",
    "\n",
    "The Part 2 is an in-class competition. \n",
    "\n",
    "Original Competition: https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge \n",
    "\n",
    "Using the file: jigsaw-toxic-comment-classification-challenge.zip\n",
    "\n",
    "The .zip file contains the Jigsaw Toxic Comment Classification dataset as provided in the original Kaggle competition. \n",
    "Data is organized as zip files containing .csv files. The training data is organized by ID, text, and label.\n",
    "\n",
    "\n",
    "Content Warning: The dataset contains text that may be considered profane, vulgar, or offensive.\n",
    "\n",
    "\n",
    "Submission Details: part2.ipynb file (Write your comments as markdown) Model and results (10 pts) + class ranking (10 pts) + report (5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dff93aa-6163-4030-a3a7-cb90b5e7bb3c",
   "metadata": {},
   "source": [
    "review -\n",
    "\n",
    "https://colab.research.google.com/github/kwanhong66/PyTorchKaggle/blob/master/Toxic_comment_classification_bert_simple.ipynb#scrollTo=y-G3zxj8zgVM\n",
    "\n",
    "refernce - \n",
    "https://towardsdatascience.com/toxic-comment-classification-using-lstm-and-lstm-cnn-db945d6b7986\n",
    "\n",
    "https://github.com/shaunak09vb/Toxic-Comment-Classifier-AWS/blob/main/source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aae8552-78ca-4ea1-be86-f82939aeda52",
   "metadata": {},
   "source": [
    "# Step 1. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a7e57f1-a69d-4ffd-941f-241388df87d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19e60bc4-3908-4f93-94dc-0a432c8a17c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df.shape = (159571, 8) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_path = \"jigsaw-toxic-comment-classification-challenge\"\n",
    "train_df = pd.read_csv(f'{base_path}/train.csv')\n",
    "\n",
    "print(f\"{train_df.shape = } \\n\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb21191c-13b1-4e15-b927-f9e5c35de25c",
   "metadata": {},
   "source": [
    "\n",
    "# Step 2. Preprocess the data as you see fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4b312ef6-cf62-46bf-9e98-40c12ac60ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-28 10:54:54--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 2600:9000:20a6:b600:13:6e38:acc0:93a1, 2600:9000:20a6:de00:13:6e38:acc0:93a1, 2600:9000:20a6:4e00:13:6e38:acc0:93a1, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|2600:9000:20a6:b600:13:6e38:acc0:93a1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 681808098 (650M) [application/zip]\n",
      "Saving to: ‘wiki-news-300d-1M.vec.zip’\n",
      "\n",
      "wiki-news-300d-1M.v 100%[===================>] 650.22M  50.1MB/s    in 12s     \n",
      "\n",
      "2024-04-28 10:55:06 (52.4 MB/s) - ‘wiki-news-300d-1M.vec.zip’ saved [681808098/681808098]\n",
      "\n",
      "Archive:  wiki-news-300d-1M.vec.zip\n",
      "  inflating: wiki-news-300d-1M.vec   \n"
     ]
    }
   ],
   "source": [
    "! mkdir model2\n",
    "! rm -rf wiki-news-300d-1M.vec.zip\n",
    "! wget  https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
    "! unzip wiki-news-300d-1M.vec.zip\n",
    "! mv wiki-news-300d-1M.vec model2/\n",
    "! rm -rf wiki-news-300d-1M.vec.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d5dd4c-d13f-4b74-a245-60d45bbd0045",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIMENSION = 300\n",
    "EMBEDDING_FILE_LOCATION = 'model/wiki-news-300d-1M.vec'\n",
    "DETECTION_CLASSES = [\n",
    "    'toxic',\n",
    "    'severe_toxic',\n",
    "    'obscene',\n",
    "    'threat',\n",
    "    'insult',\n",
    "    'identity_hate',\n",
    "    'neutral']\n",
    "\n",
    "MODEL_LOCATION = 'model/toxicity_classifier.h5'\n",
    "TOKENIZER_LOCATION = 'model/tokenizer.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9a65d54-3731-4529-8137-a29ddbef2b3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/hims/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import string\n",
    "from string import ascii_lowercase\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f9d979-1422-4d87-94ba-ae485ec7a68f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc2fc34e-838f-47a3-a5c8-68f1d63700a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import contractions\n",
    "\n",
    "def remove_contraction_(text):\n",
    "    try:\n",
    "        specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "        for s in specials:\n",
    "            text = text.replace(s, \"'\")\n",
    "        expanded_words = [] \n",
    "        for word in text.split():\n",
    "            try:\n",
    "                expanded_words.append(contractions.fix(word)) \n",
    "            except Exception as e:\n",
    "                expanded_words.append(word)\n",
    "\n",
    "        return ' '.join(expanded_words)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred during remove_contraction: {e}\")\n",
    "        print(f\"{text = }\")\n",
    "        raise e\n",
    "\n",
    "        \n",
    "def remove_contraction(text):\n",
    "    contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"'cause\": \"because\",\n",
    "                           \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
    "                           \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\", \"he'll\": \"he will\",\n",
    "                           \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\",\n",
    "                           \"how's\": \"how is\", \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\",\n",
    "                           \"I'll've\": \"I will have\", \"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\", \"i'll've\": \"i will have\", \"i'm\": \"i am\",\n",
    "                           \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\",\n",
    "                           \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\", \"let's\": \"let us\",\n",
    "                           \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\", \"mightn't\": \"might not\",\n",
    "                           \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\",\n",
    "                           \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
    "                           \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\",\n",
    "                           \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\",\n",
    "                           \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\",\n",
    "                           \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
    "                           \"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\", \"that'd\": \"that would\", \"that'd've\": \"that would have\",\n",
    "                           \"that's\": \"that is\",\n",
    "                           \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\",\n",
    "                           \"here's\": \"here is\",\n",
    "                           \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\",\n",
    "                           \"they'll've\": \"they will have\",\n",
    "                           \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\",\n",
    "                           \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\",\n",
    "                           \"we'll've\": \"we will have\",\n",
    "                           \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\",\n",
    "                           \"what'll've\": \"what will have\",\n",
    "                           \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\n",
    "                           \"when've\": \"when have\",\n",
    "                           \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\",\n",
    "                           \"who'll\": \"who will\",\n",
    "                           \"who'll've\": \"who will have\",\n",
    "                           \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\",\n",
    "                           \"will've\": \"will have\",\n",
    "                           \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\",\n",
    "                           \"wouldn't\": \"would not\",\n",
    "                           \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                           \"y'all'd've\": \"you all would have\",\n",
    "                           \"y'all're\": \"you all are\", \"y'all've\": \"you all have\", \"you'd\": \"you would\",\n",
    "                           \"you'd've\": \"you would have\", \"you'll\": \"you will\",\n",
    "                           \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    text = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in text.split(\" \")])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07404296-6e79-406c-ba86-51154a08a7dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_repeat_patterns_lower(text,remove_repeat_text=True, remove_patterns_text=True, is_lower=True):\n",
    "    \n",
    "    RE_PATTERNS = {\n",
    "        ' american ':\n",
    "            [\n",
    "                'amerikan'\n",
    "            ],\n",
    "\n",
    "        ' adolf ':\n",
    "            [\n",
    "                'adolf'\n",
    "            ],\n",
    "\n",
    "\n",
    "        ' hitler ':\n",
    "            [\n",
    "                'hitler'\n",
    "            ],\n",
    "\n",
    "        ' fuck':\n",
    "            [\n",
    "                '(f)(u|[^a-z0-9 ])(c|[^a-z0-9 ])(k|[^a-z0-9 ])([^ ])*',\n",
    "                '(f)([^a-z]*)(u)([^a-z]*)(c)([^a-z]*)(k)',\n",
    "                ' f[!@#\\$%\\^\\&\\*]*u[!@#\\$%\\^&\\*]*k', 'f u u c',\n",
    "                '(f)(c|[^a-z ])(u|[^a-z ])(k)', r'f\\*',\n",
    "                'feck ', ' fux ', 'f\\*\\*', 'f**k','fu*k',\n",
    "                'f\\-ing', 'f\\.u\\.', 'f###', ' fu ', 'f@ck', 'f u c k', 'f uck', 'f ck'\n",
    "            ],\n",
    "\n",
    "        ' ass ':\n",
    "            [\n",
    "                '[^a-z]ass ', '[^a-z]azz ', 'arrse', ' arse ', '@\\$\\$',\n",
    "                '[^a-z]anus', ' a\\*s\\*s', '[^a-z]ass[^a-z ]',\n",
    "                'a[@#\\$%\\^&\\*][@#\\$%\\^&\\*]', '[^a-z]anal ', 'a s s','a55', '@$$'\n",
    "            ],\n",
    "\n",
    "        ' ass hole ':\n",
    "            [\n",
    "                ' a[s|z]*wipe', 'a[s|z]*[w]*h[o|0]+[l]*e', '@\\$\\$hole', 'a**hole'\n",
    "            ],\n",
    "\n",
    "        ' bitch ':\n",
    "            [\n",
    "                'b[w]*i[t]*ch', 'b!tch',\n",
    "                'bi\\+ch', 'b!\\+ch', '(b)([^a-z]*)(i)([^a-z]*)(t)([^a-z]*)(c)([^a-z]*)(h)',\n",
    "                'biatch', 'bi\\*\\*h', 'bytch', 'b i t c h', 'b!tch', 'bi+ch', 'l3itch'\n",
    "            ],\n",
    "\n",
    "        ' bastard ':\n",
    "            [\n",
    "                'ba[s|z]+t[e|a]+rd'\n",
    "            ],\n",
    "\n",
    "        ' trans gender':\n",
    "            [\n",
    "                'transgender'\n",
    "            ],\n",
    "\n",
    "        ' gay ':\n",
    "            [\n",
    "                'gay'\n",
    "            ],\n",
    "\n",
    "        ' cock ':\n",
    "            [\n",
    "                '[^a-z]cock', 'c0ck', '[^a-z]cok ', 'c0k', '[^a-z]cok[^aeiou]', ' cawk',\n",
    "                '(c)([^a-z ])(o)([^a-z ]*)(c)([^a-z ]*)(k)', 'c o c k'\n",
    "            ],\n",
    "\n",
    "        ' dick ':\n",
    "            [\n",
    "                ' dick[^aeiou]', 'deek', 'd i c k', 'dik'\n",
    "            ],\n",
    "\n",
    "        ' suck ':\n",
    "            [\n",
    "                'sucker', '(s)([^a-z ]*)(u)([^a-z ]*)(c)([^a-z ]*)(k)', 'sucks', '5uck', 's u c k'\n",
    "            ],\n",
    "\n",
    "        ' cunt ':\n",
    "            [\n",
    "                'cunt', 'c u n t'\n",
    "            ],\n",
    "\n",
    "        ' bull shit ':\n",
    "            [\n",
    "                'bullsh\\*t', 'bull\\$hit'\n",
    "            ],\n",
    "\n",
    "        ' homo sex ual':\n",
    "            [\n",
    "                'homosexual'\n",
    "            ],\n",
    "\n",
    "        ' jerk ':\n",
    "            [\n",
    "                'jerk'\n",
    "            ],\n",
    "\n",
    "        ' idiot ':\n",
    "            [\n",
    "                'i[d]+io[t]+', '(i)([^a-z ]*)(d)([^a-z ]*)(i)([^a-z ]*)(o)([^a-z ]*)(t)', 'idiots'\n",
    "                                                                                          'i d i o t'\n",
    "            ],\n",
    "\n",
    "        ' dumb ':\n",
    "            [\n",
    "                '(d)([^a-z ]*)(u)([^a-z ]*)(m)([^a-z ]*)(b)'\n",
    "            ],\n",
    "\n",
    "        ' shit ':\n",
    "            [\n",
    "                'shitty', '(s)([^a-z ]*)(h)([^a-z ]*)(i)([^a-z ]*)(t)', 'shite', '\\$hit', 's h i t', '$h1t'\n",
    "            ],\n",
    "\n",
    "        ' shit hole ':\n",
    "            [\n",
    "                'shythole'\n",
    "            ],\n",
    "\n",
    "        ' retard ':\n",
    "            [\n",
    "                'returd', 'retad', 'retard', 'wiktard', 'wikitud'\n",
    "            ],\n",
    "\n",
    "        ' rape ':\n",
    "            [\n",
    "                ' raped'\n",
    "            ],\n",
    "\n",
    "        ' dumb ass':\n",
    "            [\n",
    "                'dumbass', 'dubass'\n",
    "            ],\n",
    "\n",
    "        ' ass head':\n",
    "            [\n",
    "                'butthead'\n",
    "            ],\n",
    "\n",
    "        ' sex ':\n",
    "            [\n",
    "                'sexy', 's3x', 'sexuality'\n",
    "            ],\n",
    "\n",
    "\n",
    "        ' nigger ':\n",
    "            [\n",
    "                'nigger', 'ni[g]+a', ' nigr ', 'negrito', 'niguh', 'n3gr', 'n i g g e r'\n",
    "            ],\n",
    "\n",
    "        ' shut the fuck up':\n",
    "            [\n",
    "                'stfu', 'st*u'\n",
    "            ],\n",
    "\n",
    "        ' pussy ':\n",
    "            [\n",
    "                'pussy[^c]', 'pusy', 'pussi[^l]', 'pusses', 'p*ssy'\n",
    "            ],\n",
    "\n",
    "        ' faggot ':\n",
    "            [\n",
    "                'faggot', ' fa[g]+[s]*[^a-z ]', 'fagot', 'f a g g o t', 'faggit',\n",
    "                '(f)([^a-z ]*)(a)([^a-z ]*)([g]+)([^a-z ]*)(o)([^a-z ]*)(t)', 'fau[g]+ot', 'fae[g]+ot',\n",
    "            ],\n",
    "\n",
    "        ' mother fucker':\n",
    "            [\n",
    "                ' motha ', ' motha f', ' mother f', 'motherucker',\n",
    "            ],\n",
    "\n",
    "        ' whore ':\n",
    "            [\n",
    "                'wh\\*\\*\\*', 'w h o r e'\n",
    "            ],\n",
    "        ' fucking ':\n",
    "            [\n",
    "                'f*$%-ing'\n",
    "            ],\n",
    "    }\n",
    "    \n",
    "    if is_lower:\n",
    "        text=text.lower()\n",
    "\n",
    "    if remove_patterns_text:\n",
    "        for target, patterns in RE_PATTERNS.items():\n",
    "            for pat in patterns:\n",
    "                text=str(text).replace(pat, target)\n",
    "\n",
    "    if remove_repeat_text:\n",
    "        text = re.sub(r'(.)\\1{2,}', r'\\1', text) \n",
    "\n",
    "    text = str(text).replace(\"\\n\", \" \")\n",
    "    text = re.sub(r'[^\\w\\s]',' ',text)\n",
    "    text = re.sub('[0-9]',\"\",text)\n",
    "    text = re.sub(\" +\", \" \", text)\n",
    "    text = re.sub(\"([^\\x00-\\x7F])+\",\" \",text)\n",
    "    return text \n",
    "\n",
    "\n",
    "\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  \n",
    "        u\"\\U0001F300-\\U0001F5FF\"  \n",
    "        u\"\\U0001F680-\\U0001F6FF\"  \n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "        u\"\\U0001F1F2-\\U0001F1F4\"  \n",
    "        u\"\\U0001F1E6-\\U0001F1FF\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U0001F1F2\"\n",
    "        u\"\\U0001F1F4\"\n",
    "        u\"\\U0001F620\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u2640-\\u2642\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "# from cleantext import clean\n",
    "\n",
    "# def remove_special_characters(text):\n",
    "#     # Use the clean-text library to remove emojis and special characters\n",
    "#     cleaned_text = clean(text, no_emoji=True, no_special_chars=True)\n",
    "#     return cleaned_text\n",
    "\n",
    "\n",
    "def prepare_stopwords_list():\n",
    "    stopword_list = STOP_WORDS\n",
    "    potential_stopwords = ['editor', 'reference', 'thank', 'work','find', 'good', 'know', 'like', \n",
    "                           'look', 'thing', 'want', 'time', 'list', 'section','wikipedia', 'doe', \n",
    "                           'add','new', 'try', 'think', 'write','use', 'user', 'way', 'page']\n",
    "    for word in potential_stopwords:\n",
    "        stopword_list.add(word)\n",
    "    return(stopword_list)\n",
    "\n",
    "\n",
    "def remove_stop_words(text, remove_stop=True):\n",
    "    stop_words=prepare_stopwords_list()\n",
    "    output = \"\"\n",
    "    if remove_stop:\n",
    "        text=text.split(\" \")\n",
    "        for word in text:\n",
    "            if word not in stop_words:\n",
    "                output=output + \" \" + word\n",
    "    else :\n",
    "        output=text  \n",
    "    return str(output.strip()) \n",
    "    \n",
    "\n",
    "def lemmatize(text, lemmatization=True):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    output=\"\"\n",
    "    if lemmatization:\n",
    "        text=text.split(\" \")\n",
    "        for word in text:\n",
    "            word1 = wordnet_lemmatizer.lemmatize(word, pos = \"n\")\n",
    "            word2 = wordnet_lemmatizer.lemmatize(word1, pos = \"v\")\n",
    "            word3 = wordnet_lemmatizer.lemmatize(word2, pos = \"a\")\n",
    "            word4 = wordnet_lemmatizer.lemmatize(word3, pos = \"r\")\n",
    "            output=output + \" \" + word4\n",
    "    else:\n",
    "        output=text\n",
    "    return str(output.strip())    \n",
    "\n",
    "\n",
    "# import spacy\n",
    "\n",
    "# # Load the English language model\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# def lemmatize(text):\n",
    "#     # Process the text using spaCy\n",
    "#     doc = nlp(text)\n",
    "    \n",
    "#     # Lemmatize each token in the document\n",
    "#     lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "    \n",
    "#     # Join the lemmatized tokens back into a string\n",
    "#     lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "    \n",
    "#     return lemmatized_text\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text=remove_contraction(text)\n",
    "    text=clean_repeat_patterns_lower(text)\n",
    "    text=remove_emojis(text)\n",
    "    text=remove_stop_words(text)\n",
    "    text=lemmatize(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_text_column(text_column):\n",
    "    return text_column.apply(lambda x: clean_text(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efc79f19-f73d-4a69-a299-d26d30f1752a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "class DataProcessing:\n",
    "    def __init__(self, EMBEDDING_FILE_LOCATION, DETECTION_CLASSES,\n",
    "                 max_vocab_size=100000, max_seq_length=100, embedding_dimension=300):\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.EMBEDDING_FILE_LOCATION = EMBEDDING_FILE_LOCATION\n",
    "        self.tokenizer = Tokenizer(num_words=self.max_vocab_size)\n",
    "        self.word_index = None\n",
    "        self.embedding_layer = None\n",
    "\n",
    "    def fit_tokenizer(self, data):\n",
    "        data['comment_text'] = clean_text_column(data['comment_text'])\n",
    "        self.tokenizer.fit_on_texts(data['comment_text'].values)\n",
    "        self.word_index = self.tokenizer.word_index\n",
    "        list_tokenized_train = self.tokenizer.texts_to_sequences(data['comment_text'].values)\n",
    "        X_t = pad_sequences(list_tokenized_train, maxlen=self.max_seq_length, padding='post')\n",
    "        return X_t\n",
    "        \n",
    "    def get_tokenizer(self):\n",
    "        return self.tokenizer\n",
    "\n",
    "    def get_embedding_layer(self):\n",
    "        # Load pre-trained embeddings\n",
    "        embeddings_index_fasttext = {}\n",
    "        with open(self.EMBEDDING_FILE_LOCATION, encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                embeddings_index_fasttext[word] = np.asarray(values[1:], dtype='float32')\n",
    "\n",
    "        # Create embedding matrix\n",
    "        embedding_matrix_fasttext = np.random.random((len(self.word_index) + 1, self.embedding_dimension))\n",
    "        for word, i in self.word_index.items():\n",
    "            embedding_vector = embeddings_index_fasttext.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix_fasttext[i] = embedding_vector\n",
    "\n",
    "        # Initialize embedding layer\n",
    "        self.embedding_layer = nn.Embedding(len(self.word_index) + 1, self.embedding_dimension)\n",
    "        self.embedding_layer.weight = nn.Parameter(torch.Tensor(embedding_matrix_fasttext))\n",
    "        return self.embedding_layer\n",
    "    \n",
    "    \n",
    "\n",
    "    def tokenize(self, data):\n",
    "        data['comment_text'] = clean_text_column(data['comment_text'])\n",
    "        list_tokenized = self.tokenizer.texts_to_sequences(data['comment_text'].values)\n",
    "        X_t = pad_sequences(list_tokenized, maxlen=self.max_seq_length, padding='post')\n",
    "        return X_t\n",
    "    \n",
    "    \n",
    "    def tokenize_string(self, text):\n",
    "        text = clean_text(text)\n",
    "        print(text)\n",
    "        list_tokenized = self.tokenizer.texts_to_sequences([text])\n",
    "        X_t = pad_sequences(list_tokenized, maxlen=self.max_seq_length, padding='post')\n",
    "        return X_t\n",
    "\n",
    "    \n",
    "processor = DataProcessing(EMBEDDING_FILE_LOCATION, DETECTION_CLASSES)\n",
    "X_t = processor.fit_tokenizer(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e649ad6e-355d-4efa-8e24-e53b5d0566a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "\n",
    "data_tensor = torch.tensor(X_t, dtype=torch.long)\n",
    "target_tensor = torch.tensor(train_df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values, dtype=torch.float)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(data_tensor, target_tensor, test_size=VALIDATION_SPLIT)\n",
    "\n",
    "# Create DataLoader for training and validation sets\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe1f482-c557-4a60-9896-c3e491dfd321",
   "metadata": {},
   "source": [
    "# Step 3. Utilize a model implementing a Natural Language Processing strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9ce237b-1340-4c61-9e43-3c03f9f53e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_layer, num_classes, hidden_size=40, dropout=0.1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding_layer = embedding_layer\n",
    "        self.lstm = nn.LSTM(embedding_layer.embedding_dim, hidden_size, batch_first=True)\n",
    "        self.global_max_pooling = nn.AdaptiveMaxPool1d(1)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dense1 = nn.Linear(hidden_size, 30)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dense2 = nn.Linear(30, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded_sequences = self.embedding_layer(x)\n",
    "        lstm_output, _ = self.lstm(embedded_sequences)\n",
    "        global_max_pooled = self.global_max_pooling(lstm_output.permute(0, 2, 1))\n",
    "        x = self.dropout1(global_max_pooled.squeeze(2))\n",
    "        x = self.relu(self.dense1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.dense2(x)\n",
    "        return self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9a2b81-ed86-49a5-9fdd-84922fbf5767",
   "metadata": {},
   "source": [
    "# Step 4. Train your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6912395f-d36c-4ff8-8ac5-1e44fdb8c439",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# # Given loss values\n",
    "# train_losses = [0.0638, 0.0440]\n",
    "# val_losses = [0.0492, 0.0480]\n",
    "\n",
    "# Number of epochs\n",
    "\n",
    "def plot_training_metrics(train_losses, val_losses):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "    # Plotting the training and validation loss\n",
    "    plt.plot(epochs, train_losses, 'b', label='Training Loss')\n",
    "    plt.plot(epochs, val_losses, 'r', label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68853017-7ea8-4669-86aa-23578dcec535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3990/3990 [05:41<00:00, 11.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Loss: 0.0634, Val Loss: 0.0460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3990/3990 [05:42<00:00, 11.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2, Loss: 0.0437, Val Loss: 0.0455\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # Import tqdm for progress tracking\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "def train(train_loader, val_loader, model, criterion, optimizer, num_epochs):\n",
    "    \n",
    "    print(\"Started training\")\n",
    "    train_losses, val_losses = [], []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, labels in tqdm(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                val_loss += criterion(outputs, labels).item()\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{EPOCHS}, Loss: {running_loss / len(train_loader):.4f}, Val Loss: {val_loss / len(val_loader):.4f}')\n",
    "        \n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        \n",
    "    return train_losses, val_losses, model\n",
    "\n",
    "        \n",
    "\n",
    "embedding_layer = processor.get_embedding_layer()\n",
    "model = LSTMModel(embedding_layer, num_classes=6)\n",
    "\n",
    "train_losses, val_losses, model = train(train_loader, val_loader, \n",
    "                                        model, criterion = nn.BCELoss(), \n",
    "                                        optimizer = optim.Adam(model.parameters()), \n",
    "                                        num_epochs = EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb20dad7-f5f2-4740-9253-cf8a6176f40a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a1457e8-e3d9-4984-8cb1-4968a76f1c51",
   "metadata": {},
   "source": [
    "# Step 5. Display the results of your model on the Test dataset by showing the predicted labels against their true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "012805de-254b-4e99-9312-ad0f786e3706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[38344,   793, 38344,  ...,     0,     0,     0],\n",
      "        [42475,   338,     3,  ...,     0,     0,     0],\n",
      "        [   61,  9490,  1125,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 1802,   245,   200,  ...,   200,   236, 18700],\n",
      "        [  907,    59, 11073,  ...,     0,     0,     0],\n",
      "        [    8,    17,    35,  ...,     0,     0,     0]])\n",
      "tensor([[1.2926e-03, 5.9411e-06, 3.5313e-04, 8.8784e-06, 7.6399e-05, 1.1987e-05],\n",
      "        [1.5513e-03, 5.1169e-06, 5.3402e-04, 6.3467e-06, 1.0491e-04, 1.1368e-05],\n",
      "        [1.2914e-04, 2.0728e-07, 2.3377e-05, 3.2486e-07, 4.1329e-06, 4.6442e-07],\n",
      "        [5.2545e-04, 1.4099e-06, 7.9261e-05, 3.0255e-06, 2.0635e-05, 3.4574e-06],\n",
      "        [5.5663e-04, 1.3365e-06, 1.0291e-04, 2.4775e-06, 2.5807e-05, 3.3241e-06],\n",
      "        [6.0523e-04, 1.1928e-06, 7.1045e-05, 3.3086e-06, 2.8275e-05, 3.9408e-06],\n",
      "        [3.8661e-01, 2.0375e-03, 2.5883e-02, 1.4279e-02, 9.0902e-02, 2.3587e-02],\n",
      "        [2.0242e-04, 3.4132e-07, 3.2674e-05, 6.0776e-07, 7.8310e-06, 9.1360e-07],\n",
      "        [9.7438e-05, 1.1868e-07, 1.2403e-05, 2.3945e-07, 2.7788e-06, 3.2544e-07],\n",
      "        [6.6291e-04, 1.3706e-06, 9.3205e-05, 3.2567e-06, 3.3020e-05, 4.8727e-06],\n",
      "        [1.9475e-03, 4.4275e-06, 1.9318e-04, 1.6849e-05, 1.1775e-04, 2.0341e-05],\n",
      "        [1.8318e-03, 5.6768e-06, 3.8964e-04, 1.0477e-05, 1.4549e-04, 1.9331e-05],\n",
      "        [9.5970e-04, 3.0349e-06, 1.4010e-04, 7.0235e-06, 4.7978e-05, 8.4812e-06],\n",
      "        [9.9720e-01, 4.5930e-01, 9.8607e-01, 4.1064e-02, 8.7526e-01, 1.4587e-01],\n",
      "        [3.4653e-04, 8.6002e-07, 5.9276e-05, 1.5287e-06, 1.4865e-05, 2.0214e-06],\n",
      "        [1.0080e-01, 8.3350e-04, 1.1050e-02, 3.1722e-03, 1.2821e-02, 3.3989e-03],\n",
      "        [1.4107e-03, 4.4870e-06, 2.2887e-04, 1.0451e-05, 7.8662e-05, 1.2656e-05],\n",
      "        [3.5712e-04, 8.6641e-07, 7.0242e-05, 1.4320e-06, 1.6260e-05, 2.1613e-06],\n",
      "        [6.5787e-03, 4.2267e-05, 1.4865e-03, 9.2618e-05, 5.3852e-04, 1.1142e-04],\n",
      "        [1.2936e-03, 3.4974e-06, 1.5150e-04, 1.0564e-05, 6.9988e-05, 1.2842e-05],\n",
      "        [8.9956e-04, 3.6361e-06, 1.6294e-04, 7.1163e-06, 4.5737e-05, 8.5217e-06],\n",
      "        [2.8329e-03, 1.9356e-05, 4.3805e-04, 4.9247e-05, 1.9786e-04, 5.1264e-05],\n",
      "        [4.0624e-02, 6.2538e-04, 5.6716e-03, 1.8400e-03, 4.7551e-03, 1.6083e-03],\n",
      "        [1.2425e-03, 2.0597e-06, 1.6516e-04, 5.5465e-06, 7.5335e-05, 9.4504e-06],\n",
      "        [3.9888e-03, 1.9871e-05, 5.2088e-04, 6.0462e-05, 2.5254e-04, 5.7205e-05],\n",
      "        [1.7396e-03, 5.9551e-06, 2.4133e-04, 1.6517e-05, 1.0178e-04, 1.9186e-05],\n",
      "        [9.7988e-01, 1.7253e-01, 9.1581e-01, 3.7210e-02, 7.1299e-01, 1.1076e-01],\n",
      "        [6.0455e-04, 1.0594e-06, 6.1659e-05, 3.2796e-06, 2.9762e-05, 4.3844e-06],\n",
      "        [1.2339e-03, 1.3217e-06, 1.1171e-04, 4.8014e-06, 6.8567e-05, 8.4257e-06],\n",
      "        [3.0724e-02, 1.1124e-03, 5.3440e-03, 3.3816e-03, 6.9554e-03, 3.6981e-03],\n",
      "        [1.1374e-02, 7.3204e-05, 1.2214e-03, 3.0044e-04, 1.1111e-03, 3.0403e-04],\n",
      "        [1.2924e-03, 3.9882e-06, 1.7212e-04, 1.0416e-05, 6.5765e-05, 1.1688e-05]])\n",
      "tensor([1.2926e-03, 5.9411e-06, 3.5313e-04, 8.8784e-06, 7.6399e-05, 1.1987e-05]) \n",
      " tensor([0., 0., 0., 0., 0., 0.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hims/anaconda3/envs/python310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/hims/anaconda3/envs/python310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Class  Precision    Recall  F1-score  Accuracy   ROC AUC\n",
      "0          toxic   0.833090  0.764037  0.797071  0.963528  0.977809\n",
      "1   severe_toxic   0.558011  0.305136  0.394531  0.990287  0.990240\n",
      "2        obscene   0.819129  0.816626  0.817876  0.981357  0.992261\n",
      "3         threat   0.000000  0.000000  0.000000  0.996835  0.969773\n",
      "4         insult   0.742055  0.658361  0.697707  0.972740  0.982429\n",
      "5  identity_hate   0.000000  0.000000  0.000000  0.992229  0.972011\n",
      "6        average   0.492048  0.424027  0.451197  0.982829  0.980754\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    predicted = []\n",
    "    actual = []\n",
    "    flag = False\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            outputs = model(inputs)\n",
    "            predicted.extend(outputs)\n",
    "            actual.extend(labels)\n",
    "            if flag:\n",
    "                flag = False\n",
    "                print(inputs)\n",
    "                print(outputs)\n",
    "                \n",
    "    print(predicted[0], '\\n', actual[0])\n",
    "\n",
    "    threshold = 0.5\n",
    "    predicted_classes = [[1 if val >= threshold else 0 for val in output] for output in predicted]\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    accuracy = []\n",
    "    roc_auc = []\n",
    "    for i in range(len(actual[0])):\n",
    "        precision.append(precision_score([row[i] for row in actual], [row[i] for row in predicted_classes]))\n",
    "        recall.append(recall_score([row[i] for row in actual], [row[i] for row in predicted_classes]))\n",
    "        f1.append(f1_score([row[i] for row in actual], [row[i] for row in predicted_classes]))\n",
    "        accuracy.append(accuracy_score([row[i] for row in actual], [row[i] for row in predicted_classes]))\n",
    "        roc_auc.append(roc_auc_score([row[i] for row in actual], [row[i] for row in predicted]))\n",
    "\n",
    "    # Create a DataFrame to display the metrics\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Class': [DETECTION_CLASSES[i] for i in list(range(len(actual[0])))] + ['average'],\n",
    "        'Precision': precision + [np.mean(precision)],\n",
    "        'Recall': recall + [np.mean(recall)],\n",
    "        'F1-score': f1 + [np.mean(f1)],\n",
    "        'Accuracy': accuracy + [np.mean(accuracy)],\n",
    "        'ROC AUC': roc_auc + [np.mean(roc_auc)]\n",
    "    })\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(metrics_df.head(10))\n",
    "    \n",
    "    \n",
    "evaluate(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fc3fc29e-d0a7-4ff7-8f6d-f160a9695a3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test shape =  (63978, 8)\n",
      "                 id  \\\n",
      "0  0001ea8717f6de06   \n",
      "1  000247e83dcc1211   \n",
      "2  0002f87b16116a7f   \n",
      "3  0003e1cccfd5a40a   \n",
      "4  00059ace3e3e9a53   \n",
      "5  000663aff0fffc80   \n",
      "6  000689dd34e20979   \n",
      "7  000844b52dee5f3f   \n",
      "8  00091c35fa9d0465   \n",
      "9  000968ce11f5ee34   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   comment_text  \\\n",
      "0                                                                                                                                                                                                                                                                                                                                                                                                                              Thank you for understanding. I think very highly of you and would not revert without discussion.   \n",
      "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              :Dear god this site is horrible.   \n",
      "2                                                           \"::: Somebody will invariably try to add Religion?  Really??  You mean, the way people have invariably kept adding \"\"Religion\"\" to the Samuel Beckett infobox?  And why do you bother bringing up the long-dead completely non-existent \"\"Influences\"\" issue?  You're just flailing, making up crap on the fly. \\n ::: For comparison, the only explicit acknowledgement in the entire Amos Oz article that he is personally Jewish is in the categories!    \\n\\n \"   \n",
      "3  \" \\n\\n It says it right there that it IS a type. The \"\"Type\"\" of institution is needed in this case because there are three levels of SUNY schools: \\n -University Centers and Doctoral Granting Institutions \\n -State Colleges \\n -Community Colleges. \\n\\n It is needed in this case to clarify that UB is a SUNY Center. It says it even in Binghamton University, University at Albany, State University of New York, and Stony Brook University. Stop trying to say it's not because I am totally right in this case.\"   \n",
      "4                                                                                                                                                                           \" \\n\\n == Before adding a new product to the list, make sure it's relevant == \\n\\n Before adding a new product to the list, make sure it has a wikipedia entry already, \"\"proving\"\" it's relevance and giving the reader the possibility to read more about it. \\n Otherwise it could be subject to deletion. See this article's revision history.\"   \n",
      "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      this other one from 1897   \n",
      "6                                                                                                                                                                                                                                                                                                                                                             == Reason for banning throwing == \\n\\n This article needs a section on /why/ throwing is banned. At the moment, to a non-cricket fan, it seems kind of arbitrary.   \n",
      "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |blocked]] from editing Wikipedia.   |   \n",
      "8                                                                                                                                                                                                                                                                                                                                                                                                                    == Arabs are committing genocide in Iraq, but no protests in Europe. == \\n\\n May Europe also burn in hell.   \n",
      "9                                                                                                                                                                                                                                                                                                                                                                                                           Please stop. If you continue to vandalize Wikipedia, as you did to Homosexuality, you will be blocked from editing.   \n",
      "\n",
      "   toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
      "0      0             0        0       0       0              0  \n",
      "1      0             0        0       0       0              0  \n",
      "2      0             0        0       0       0              0  \n",
      "3      0             0        0       0       0              0  \n",
      "4      0             0        0       0       0              0  \n",
      "5      0             0        0       0       0              0  \n",
      "6      0             0        0       0       0              0  \n",
      "7      0             0        0       0       0              0  \n",
      "8      1             0        0       0       0              0  \n",
      "9      0             0        0       0       0              0  \n",
      "tensor([[ 116,  654,   55,  ...,    0,    0,    0],\n",
      "        [  13,    3, 1165,  ...,    0,    0,    0],\n",
      "        [   0,    0,    0,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 817,  847, 1319,  ...,    0,    0,    0],\n",
      "        [  93,  272,   36,  ...,    0,    0,    0],\n",
      "        [ 205,  205,  618,  ...,    0,    0,    0]])\n",
      "tensor([[3.9417e-04, 4.1574e-07, 3.9303e-05, 1.2174e-06, 1.6391e-05, 1.9542e-06],\n",
      "        [4.6002e-03, 1.2528e-05, 4.8833e-04, 4.9874e-05, 3.1462e-04, 5.8068e-05],\n",
      "        [1.6050e-02, 1.8860e-04, 1.7401e-03, 7.5775e-04, 1.4245e-03, 4.8665e-04],\n",
      "        [8.8982e-01, 7.1663e-03, 7.4047e-01, 2.7769e-03, 2.7419e-01, 1.0507e-02],\n",
      "        [3.1990e-03, 1.3035e-05, 4.1613e-04, 4.0087e-05, 2.1916e-04, 4.4721e-05],\n",
      "        [3.9872e-04, 5.4800e-07, 4.0572e-05, 1.6399e-06, 1.6661e-05, 2.1868e-06],\n",
      "        [3.4815e-02, 3.6570e-04, 4.9196e-03, 1.2782e-03, 4.7180e-03, 1.2530e-03],\n",
      "        [8.6848e-04, 3.9953e-06, 1.2574e-04, 9.0342e-06, 6.5901e-05, 1.3837e-05],\n",
      "        [3.8529e-03, 3.0010e-05, 6.1466e-04, 7.9231e-05, 2.8216e-04, 7.4087e-05],\n",
      "        [4.2684e-03, 2.1031e-05, 5.6621e-04, 6.5996e-05, 3.2497e-04, 7.2632e-05],\n",
      "        [6.8488e-04, 1.5752e-06, 9.5170e-05, 3.7819e-06, 3.3760e-05, 5.1124e-06],\n",
      "        [3.8356e-03, 1.3734e-05, 1.5948e-03, 1.5377e-05, 2.9706e-04, 3.0779e-05],\n",
      "        [2.4043e-02, 6.8889e-04, 6.4214e-03, 1.3303e-03, 2.9493e-03, 1.0434e-03],\n",
      "        [3.4554e-03, 1.5370e-05, 4.7299e-04, 4.5685e-05, 2.3910e-04, 5.1300e-05],\n",
      "        [8.9256e-01, 1.5379e-02, 7.2538e-01, 6.5704e-03, 3.3222e-01, 2.1276e-02],\n",
      "        [2.1277e-03, 3.0181e-06, 2.0560e-04, 1.1616e-05, 1.2726e-04, 1.7008e-05],\n",
      "        [5.5928e-01, 2.1891e-03, 9.7582e-02, 7.0680e-03, 1.4202e-01, 1.8012e-02],\n",
      "        [3.3875e-04, 3.4279e-07, 3.6392e-05, 9.2712e-07, 1.2332e-05, 1.3713e-06],\n",
      "        [2.2009e-01, 2.1037e-04, 7.6459e-02, 3.7081e-04, 2.2414e-02, 1.1314e-03],\n",
      "        [4.5037e-03, 5.5633e-06, 4.9885e-04, 2.0678e-05, 3.8147e-04, 4.4818e-05],\n",
      "        [3.4478e-04, 9.5121e-07, 6.3267e-05, 1.6346e-06, 1.3715e-05, 2.1154e-06],\n",
      "        [6.5748e-03, 1.1255e-05, 1.0975e-03, 3.0191e-05, 5.4600e-04, 6.0696e-05],\n",
      "        [9.8623e-01, 7.5566e-02, 9.6012e-01, 7.2667e-03, 6.0490e-01, 2.8467e-02],\n",
      "        [3.1717e-03, 1.7142e-05, 7.1242e-04, 3.2197e-05, 2.2471e-04, 4.2299e-05],\n",
      "        [8.7162e-01, 4.6446e-03, 7.1897e-01, 1.6419e-03, 2.0897e-01, 6.4926e-03],\n",
      "        [4.6241e-03, 1.0500e-05, 4.2406e-04, 4.2767e-05, 2.7264e-04, 5.0082e-05],\n",
      "        [5.0997e-03, 6.6233e-06, 4.6255e-04, 2.8380e-05, 4.0416e-04, 5.4230e-05],\n",
      "        [4.1933e-04, 1.0236e-06, 6.4363e-05, 2.0680e-06, 1.7286e-05, 2.6781e-06],\n",
      "        [1.7272e-04, 3.0881e-07, 2.7383e-05, 5.5424e-07, 6.2113e-06, 7.8957e-07],\n",
      "        [5.4620e-03, 2.6647e-05, 7.4098e-04, 8.1852e-05, 4.3763e-04, 9.4469e-05],\n",
      "        [2.0150e-04, 2.2825e-07, 2.1507e-05, 5.9839e-07, 6.9152e-06, 8.2164e-07],\n",
      "        [7.0567e-05, 1.2342e-07, 1.3043e-05, 1.7036e-07, 1.9964e-06, 2.3880e-07]])\n",
      "tensor([3.9417e-04, 4.1574e-07, 3.9303e-05, 1.2174e-06, 1.6391e-05, 1.9542e-06]) \n",
      " tensor([0., 0., 0., 0., 0., 0.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hims/anaconda3/envs/python310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/hims/anaconda3/envs/python310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Class  Precision    Recall  F1-score  Accuracy   ROC AUC\n",
      "0          toxic   0.549460  0.843678  0.665501  0.919269  0.964756\n",
      "1   severe_toxic   0.428571  0.465940  0.446475  0.993373  0.988913\n",
      "2        obscene   0.604373  0.793823  0.686263  0.958126  0.978355\n",
      "3         threat   0.000000  0.000000  0.000000  0.996702  0.965655\n",
      "4         insult   0.616347  0.618325  0.617334  0.958939  0.967661\n",
      "5  identity_hate   0.000000  0.000000  0.000000  0.988871  0.967116\n",
      "6        average   0.366459  0.453628  0.402596  0.969213  0.972076\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv(f'jigsaw-toxic-comment-classification-challenge/test.csv')\n",
    "df2 = pd.read_csv(f'jigsaw-toxic-comment-classification-challenge/test_labels.csv')\n",
    "\n",
    "df_test = pd.merge(df1, df2, on='id', how='inner')\n",
    "\n",
    "# removing sample with labels equal to -1\n",
    "df_test = df_test.loc[df_test['toxic'] >= 0]\n",
    "df_test.reset_index(inplace=True)\n",
    "df_test = df_test.drop(columns=['index'])\n",
    "# print(df_test.head())\n",
    "print(\"test shape = \", df_test.shape)\n",
    "# print(df_test.head(10))\n",
    "\n",
    "\n",
    "# df_test['comment_text'] = clean_text_column(df_test['comment_text'])\n",
    "X_test = processor.tokenize(df_test)\n",
    "\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "test_data_tensor = torch.tensor(X_test, dtype=torch.long)\n",
    "test_target_tensor = torch.tensor(df_test[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values, dtype=torch.float)\n",
    "\n",
    "test_dataset = TensorDataset(test_data_tensor, test_target_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "86840dea-dbab-40e8-9da2-716718d993e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_labels(text):\n",
    "    seq = processor.tokenize_string(text)\n",
    "    # print(seq)\n",
    "    predicted = model(torch.tensor(seq))\n",
    "    # print(predicted)\n",
    "    predicted_classes = [[1 if val >= 0.5 else 0 for val in output] for output in predicted]\n",
    "    # print(predicted_classes[0])\n",
    "    return [DETECTION_CLASSES[i] for i, x in enumerate(predicted_classes[0]) if x == 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e31acc81-56bf-4e55-a8e7-e31a9a24bc6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj robinson gay hell suck dick\n",
      "fuck anti semitic cunt\n",
      "arrogant self serve immature idiot right\n",
      "suck university nicknameless ncaa history happen\n",
      "argh random idiot delete japan article\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>actual_labels</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>DJ Robinson is gay as hell! he sucks his dick so much!!!!!</td>\n",
       "      <td>toxic,obscene,insult,identity_hate</td>\n",
       "      <td>[toxic, obscene, insult]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>:Fuck off, you anti-semitic cunt.  |</td>\n",
       "      <td>toxic,obscene,insult</td>\n",
       "      <td>[toxic, obscene, insult]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>::No, he is an arrogant, self serving, immature idiot. Get it right.</td>\n",
       "      <td>toxic,obscene,insult</td>\n",
       "      <td>[toxic, insult]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>::::Well, it sucks to have a university to be nicknameless. And it's the first time in NCAA history that it has happened. /</td>\n",
       "      <td>toxic,obscene</td>\n",
       "      <td>[toxic, obscene]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>== Argh == \\n\\n Some random idiot deleted the whole Japan article &lt;.</td>\n",
       "      <td>toxic,obscene,insult</td>\n",
       "      <td>[toxic]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                    comment_text  \\\n",
       "24                                                                    DJ Robinson is gay as hell! he sucks his dick so much!!!!!   \n",
       "29                                                                                          :Fuck off, you anti-semitic cunt.  |   \n",
       "40                                                          ::No, he is an arrogant, self serving, immature idiot. Get it right.   \n",
       "106  ::::Well, it sucks to have a university to be nicknameless. And it's the first time in NCAA history that it has happened. /   \n",
       "107                                                         == Argh == \\n\\n Some random idiot deleted the whole Japan article <.   \n",
       "\n",
       "                          actual_labels           predicted_label  \n",
       "24   toxic,obscene,insult,identity_hate  [toxic, obscene, insult]  \n",
       "29                 toxic,obscene,insult  [toxic, obscene, insult]  \n",
       "40                 toxic,obscene,insult           [toxic, insult]  \n",
       "106                       toxic,obscene          [toxic, obscene]  \n",
       "107                toxic,obscene,insult                   [toxic]  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(f'jigsaw-toxic-comment-classification-challenge/test.csv')\n",
    "df2 = pd.read_csv(f'jigsaw-toxic-comment-classification-challenge/test_labels.csv')\n",
    "\n",
    "df_test = pd.merge(df1, df2, on='id', how='inner')\n",
    "\n",
    "# removing sample with labels equal to -1\n",
    "df_test = df_test.loc[df_test['toxic'] >= 0]\n",
    "df_test.reset_index(inplace=True)\n",
    "df_test = df_test.drop(columns=['index'])\n",
    "\n",
    "\n",
    "# Filter rows where sum of labels is greater than 0\n",
    "filtered_df = df_test[(df_test.iloc[:, 2:].sum(axis=1) > 1)].copy()\n",
    "\n",
    "# Concatenate labels into a single string using apply and lambda\n",
    "filtered_df['labels'] = filtered_df.apply(lambda row: ','.join(row.index[2:][row[2:] > 0]), axis=1)\n",
    "\n",
    "# Create new DataFrame with comment_text and labels\n",
    "selected_df = filtered_df[['comment_text', 'labels']].head(5)\n",
    "selected_df.rename(columns={'labels': 'actual_labels'}, inplace=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "selected_df['predicted_label'] = selected_df['comment_text'].apply(get_labels)\n",
    "\n",
    "\n",
    "selected_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40017465-32da-4403-a488-80ee4d04437c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
