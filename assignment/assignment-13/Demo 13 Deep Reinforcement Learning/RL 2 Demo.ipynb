{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d635edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\jayam\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\jayam\\anaconda3\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\jayam\\anaconda3\\lib\\site-packages (from torch) (3.10.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jayam\\anaconda3\\lib\\site-packages\\matplotlib\\__init__.py:152: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(module.__version__) < minver:\n",
      "C:\\Users\\jayam\\anaconda3\\lib\\site-packages\\setuptools\\_distutils\\version.py:351: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1108c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.01\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "n_episodes = 100000\n",
    "hidden_dim = 12\n",
    "min_eps = 0.01\n",
    "max_eps_episode = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46d6431b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class QNN(nn.Module):\n",
    "    def __init__(self, ip_dim, op_dim, hid_dim) -> None:\n",
    "        \n",
    "        super(QNN, self).__init__()\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(ip_dim, hid_dim),\n",
    "            torch.nn.PReLU()\n",
    "        )\n",
    "\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hid_dim, hid_dim),\n",
    "            torch.nn.PReLU()\n",
    "        )\n",
    "\n",
    "        self.layer3 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hid_dim, hid_dim),\n",
    "            torch.nn.PReLU()\n",
    "        )\n",
    "        \n",
    "        self.final = torch.nn.Linear(hid_dim, op_dim)\n",
    "        \n",
    "    def forward(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        t = self.layer1(t)\n",
    "        t = self.layer2(t)\n",
    "        t = self.layer3(t)\n",
    "        t = self.final(t)\n",
    "\n",
    "        return t\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "106292d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import namedtuple\n",
    "\n",
    "transition = namedtuple('transition',('state', 'action', 'reward','nxt_state','stop'))\n",
    "\n",
    "class ReplayMemory_CyclicBuffer(object):\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity) \n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "        \n",
    "    def push_to_memory(self, *args):\n",
    "        self.memory.append(transition(*args))\n",
    "        \n",
    "    def random_sampling(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    \n",
    "# test = transition(((1,2),(2,3),(4,5)),('stay','left','right'),(1,1,1),((4,5),(6,7),(8,9)))\n",
    "# print(test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f49c9a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "FloatTensor = torch.FloatTensor #32-bit float multi dimension matrix\n",
    "\n",
    "class Agent_MountainCart(object):\n",
    "    \n",
    "    def __init__(self, n_states, n_actions, hidden_dim, learning_rate):\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        self.local_q = QNN(n_states, n_actions, hidden_dim).to(self.device)\n",
    "        self.target_q = QNN(n_states, n_actions, hidden_dim).to(self.device)\n",
    "        \n",
    "        self.mse_loss = torch.nn.MSELoss()\n",
    "        self.optim = optim.Adam(self.local_q.parameters(), lr=learning_rate)\n",
    "        \n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        self.replay_memory = ReplayMemory_CyclicBuffer(10000)\n",
    "        \n",
    "        \n",
    "    def get_action(self, state, eps_greedy, eps_flag = True):\n",
    "        global steps_done\n",
    "        \n",
    "        random_sample = random.random()\n",
    "        \n",
    "        if random_sample > eps_greedy:\n",
    "            with torch.no_grad():\n",
    "                return self.local_q((state).type(FloatTensor)).data.max(1)[1].view(1,1)\n",
    "        else:\n",
    "            return torch.tensor([[random.randrange(self.n_actions)]])\n",
    "        \n",
    "    def learn(self, experience, GAMMA):\n",
    "        \n",
    "        if len(self.replay_memory.memory) < BATCH_SIZE:\n",
    "            return;\n",
    "        \n",
    "        transitions_sample = self.replay_memory.random_sampling(BATCH_SIZE)\n",
    "        batch = transition(*zip(*transitions_sample))\n",
    "        \n",
    "        states = torch.cat(batch.state)\n",
    "        actions = torch.cat(batch.action)\n",
    "        rewards = torch.cat(batch.reward)\n",
    "        nxt_states = torch.cat(batch.nxt_state)\n",
    "        stops = torch.cat(batch.stop)\n",
    "        \n",
    "        expected_Q = torch.gather(self.local_q(states),1,actions)\n",
    "        nxt_targets_Q = self.target_q(nxt_states).detach().max(1)[0]\n",
    "        targets_Q = rewards + (GAMMA * nxt_targets_Q * (1-stops)) #1-dones\n",
    "        self.local_q.train(mode = True)\n",
    "        self.optim.zero_grad()\n",
    "        loss = self.mse_loss(expected_Q, targets_Q.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d49ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e216e826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ip_dim: 2 op_dim: 3 hidden_dim: 12 threshold: -110.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jayam\\anaconda3\\lib\\site-packages\\gym\\core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "C:\\Users\\jayam\\anaconda3\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "dim_space = env.observation_space.shape[0]\n",
    "dim_action = env.action_space.n\n",
    "\n",
    "threshold = env.spec.reward_threshold\n",
    "\n",
    "print('ip_dim:', dim_space, 'op_dim:', dim_action,'hidden_dim:',hidden_dim,'threshold:', threshold)\n",
    "\n",
    "agent = Agent_MountainCart(dim_space, dim_action, hidden_dim, learning_rate=LEARNING_RATE)\n",
    "\n",
    "def epsilon_annealing(i_episode, max_episode, min_eps: float):\n",
    "    \n",
    "    slope = (min_eps - 1.0)/max_episode\n",
    "    max_eps = max(slope * i_episode + 1.0, min_eps)\n",
    "    return max_eps\n",
    "\n",
    "def save_q(directory, filename):\n",
    "    torch.save(agent.local_q.state_dict(), '%s/%s_local.pth' % (directory, filename))\n",
    "    torch.save(agent.target_q.state_dict(), '%s/%s_target.pth' % (directory, filename))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd0e318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode(env, agent, eps):\n",
    "    \n",
    "    state = env.reset()\n",
    "    stop = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not stop:\n",
    "        action = agent.get_action(FloatTensor([state]), eps)\n",
    "        nxt_state, reward, stop, _ = env.step(action.item())\n",
    "        total_reward = total_reward + reward\n",
    "        \n",
    "        if stop:\n",
    "            reward = -1\n",
    "            \n",
    "        agent.replay_memory.push_to_memory(\n",
    "            FloatTensor([state]),\n",
    "             action,\n",
    "             FloatTensor([reward]),\n",
    "             FloatTensor([nxt_state]),\n",
    "             FloatTensor([stop])\n",
    "        )\n",
    "        \n",
    "        if(len(agent.replay_memory) > BATCH_SIZE):\n",
    "            batch = agent.replay_memory.random_sampling(BATCH_SIZE)\n",
    "            agent.learn(batch, GAMMA)\n",
    "            \n",
    "        state = nxt_state\n",
    "        \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e4dac48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jayam\\AppData\\Local\\Temp/ipykernel_1356/437941733.py:8: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:204.)\n",
      "  action = agent.get_action(FloatTensor([state]), eps)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:    10 Score: -200.0  Avg.Score: -200.00, eps-greedy: 0.802 Time: 00:00:04\n",
      "Episode:    20 Score: -200.0  Avg.Score: -200.00, eps-greedy: 0.604 Time: 00:00:09\n",
      "Episode:    30 Score: -200.0  Avg.Score: -200.00, eps-greedy: 0.406 Time: 00:00:13\n",
      "Episode:    40 Score: -200.0  Avg.Score: -200.00, eps-greedy: 0.208 Time: 00:00:18\n",
      "Episode:    50 Score: -200.0  Avg.Score: -198.39, eps-greedy: 0.010 Time: 00:00:23\n",
      "Episode:    60 Score: -200.0  Avg.Score: -198.66, eps-greedy: 0.010 Time: 00:00:29\n",
      "Episode:    70 Score: -200.0  Avg.Score: -197.37, eps-greedy: 0.010 Time: 00:00:35\n",
      "Episode:    80 Score: -200.0  Avg.Score: -195.04, eps-greedy: 0.010 Time: 00:00:40\n",
      "Episode:    90 Score: -117.0  Avg.Score: -194.67, eps-greedy: 0.010 Time: 00:00:46\n",
      "Episode:   100 Score: -200.0  Avg.Score: -193.08, eps-greedy: 0.010 Time: 00:00:54\n",
      "Episode:   110 Score: -200.0  Avg.Score: -193.08, eps-greedy: 0.010 Time: 00:01:01\n",
      "Episode:   120 Score: -200.0  Avg.Score: -193.08, eps-greedy: 0.010 Time: 00:01:07\n",
      "Episode:   130 Score: -197.0  Avg.Score: -191.99, eps-greedy: 0.010 Time: 00:01:13\n",
      "Episode:   140 Score: -93.0  Avg.Score: -190.92, eps-greedy: 0.010 Time: 00:01:19\n",
      "Episode:   150 Score: -200.0  Avg.Score: -190.57, eps-greedy: 0.010 Time: 00:01:23\n",
      "Episode:   160 Score: -200.0  Avg.Score: -189.49, eps-greedy: 0.010 Time: 00:01:28\n",
      "Episode:   170 Score: -200.0  Avg.Score: -189.55, eps-greedy: 0.010 Time: 00:01:32\n",
      "Episode:   180 Score: -200.0  Avg.Score: -190.88, eps-greedy: 0.010 Time: 00:01:36\n",
      "Episode:   190 Score: -200.0  Avg.Score: -191.71, eps-greedy: 0.010 Time: 00:01:41\n",
      "Episode:   200 Score: -200.0  Avg.Score: -192.67, eps-greedy: 0.010 Time: 00:01:45\n",
      "Episode:   210 Score: -200.0  Avg.Score: -191.49, eps-greedy: 0.010 Time: 00:01:49\n",
      "Episode:   220 Score: -200.0  Avg.Score: -190.26, eps-greedy: 0.010 Time: 00:01:53\n",
      "Episode:   230 Score: -200.0  Avg.Score: -190.35, eps-greedy: 0.010 Time: 00:01:57\n",
      "Episode:   240 Score: -200.0  Avg.Score: -190.15, eps-greedy: 0.010 Time: 00:02:02\n",
      "Episode:   250 Score: -200.0  Avg.Score: -187.85, eps-greedy: 0.010 Time: 00:02:05\n",
      "Episode:   260 Score: -95.0  Avg.Score: -187.21, eps-greedy: 0.010 Time: 00:02:09\n",
      "Episode:   270 Score: -190.0  Avg.Score: -185.53, eps-greedy: 0.010 Time: 00:02:13\n",
      "Episode:   280 Score: -200.0  Avg.Score: -186.10, eps-greedy: 0.010 Time: 00:02:17\n",
      "Episode:   290 Score: -200.0  Avg.Score: -182.70, eps-greedy: 0.010 Time: 00:02:21\n",
      "Episode:   300 Score: -200.0  Avg.Score: -181.40, eps-greedy: 0.010 Time: 00:02:25\n",
      "Episode:   310 Score: -200.0  Avg.Score: -180.20, eps-greedy: 0.010 Time: 00:02:29\n",
      "Episode:   320 Score: -200.0  Avg.Score: -178.47, eps-greedy: 0.010 Time: 00:02:33\n",
      "Episode:   330 Score: -200.0  Avg.Score: -178.12, eps-greedy: 0.010 Time: 00:02:37\n",
      "Episode:   340 Score: -200.0  Avg.Score: -179.39, eps-greedy: 0.010 Time: 00:02:41\n",
      "Episode:   350 Score: -200.0  Avg.Score: -181.71, eps-greedy: 0.010 Time: 00:02:45\n",
      "Episode:   360 Score: -200.0  Avg.Score: -180.64, eps-greedy: 0.010 Time: 00:02:49\n",
      "Episode:   370 Score: -200.0  Avg.Score: -180.07, eps-greedy: 0.010 Time: 00:02:53\n",
      "Episode:   380 Score: -200.0  Avg.Score: -177.09, eps-greedy: 0.010 Time: 00:02:57\n",
      "Episode:   390 Score: -200.0  Avg.Score: -180.28, eps-greedy: 0.010 Time: 00:03:01\n",
      "Episode:   400 Score: -200.0  Avg.Score: -179.38, eps-greedy: 0.010 Time: 00:03:05\n",
      "Episode:   410 Score: -200.0  Avg.Score: -178.53, eps-greedy: 0.010 Time: 00:03:08\n",
      "Episode:   420 Score: -200.0  Avg.Score: -178.84, eps-greedy: 0.010 Time: 00:03:12\n",
      "Episode:   430 Score: -100.0  Avg.Score: -176.98, eps-greedy: 0.010 Time: 00:03:16\n",
      "Episode:   440 Score: -200.0  Avg.Score: -174.62, eps-greedy: 0.010 Time: 00:03:20\n",
      "Episode:   450 Score: -200.0  Avg.Score: -174.78, eps-greedy: 0.010 Time: 00:03:24\n",
      "Episode:   460 Score: -200.0  Avg.Score: -169.93, eps-greedy: 0.010 Time: 00:03:29\n",
      "Episode:   470 Score: -200.0  Avg.Score: -172.16, eps-greedy: 0.010 Time: 00:03:35\n",
      "Episode:   480 Score: -200.0  Avg.Score: -172.30, eps-greedy: 0.010 Time: 00:03:40\n",
      "Episode:   490 Score: -200.0  Avg.Score: -169.92, eps-greedy: 0.010 Time: 00:03:43\n",
      "Episode:   500 Score: -200.0  Avg.Score: -169.52, eps-greedy: 0.010 Time: 00:03:47\n",
      "Episode:   510 Score: -200.0  Avg.Score: -167.01, eps-greedy: 0.010 Time: 00:03:50\n",
      "Episode:   520 Score: -200.0  Avg.Score: -166.24, eps-greedy: 0.010 Time: 00:03:54\n",
      "Episode:   530 Score: -200.0  Avg.Score: -168.34, eps-greedy: 0.010 Time: 00:03:58\n",
      "Episode:   540 Score: -185.0  Avg.Score: -167.85, eps-greedy: 0.010 Time: 00:04:02\n",
      "Episode:   550 Score: -200.0  Avg.Score: -163.28, eps-greedy: 0.010 Time: 00:04:05\n",
      "Episode:   560 Score: -100.0  Avg.Score: -165.92, eps-greedy: 0.010 Time: 00:04:09\n",
      "Episode:   570 Score: -88.0  Avg.Score: -161.04, eps-greedy: 0.010 Time: 00:04:12\n",
      "Episode:   580 Score: -179.0  Avg.Score: -159.32, eps-greedy: 0.010 Time: 00:04:15\n",
      "Episode:   590 Score: -200.0  Avg.Score: -155.93, eps-greedy: 0.010 Time: 00:04:18\n",
      "Episode:   600 Score: -78.0  Avg.Score: -154.30, eps-greedy: 0.010 Time: 00:04:21\n",
      "Episode:   610 Score: -200.0  Avg.Score: -155.22, eps-greedy: 0.010 Time: 00:04:25\n",
      "Episode:   620 Score: -81.0  Avg.Score: -151.47, eps-greedy: 0.010 Time: 00:04:27\n",
      "Episode:   630 Score: -78.0  Avg.Score: -146.79, eps-greedy: 0.010 Time: 00:04:31\n",
      "Episode:   640 Score: -200.0  Avg.Score: -142.94, eps-greedy: 0.010 Time: 00:04:33\n",
      "Episode:   650 Score: -171.0  Avg.Score: -143.35, eps-greedy: 0.010 Time: 00:04:37\n",
      "Episode:   660 Score: -176.0  Avg.Score: -142.73, eps-greedy: 0.010 Time: 00:04:40\n",
      "Episode:   670 Score: -200.0  Avg.Score: -144.04, eps-greedy: 0.010 Time: 00:04:43\n",
      "Episode:   680 Score: -170.0  Avg.Score: -141.22, eps-greedy: 0.010 Time: 00:04:46\n",
      "Episode:   690 Score: -200.0  Avg.Score: -142.95, eps-greedy: 0.010 Time: 00:04:49\n",
      "Episode:   700 Score: -152.0  Avg.Score: -141.08, eps-greedy: 0.010 Time: 00:04:52\n",
      "Episode:   710 Score: -200.0  Avg.Score: -138.50, eps-greedy: 0.010 Time: 00:04:55\n",
      "Episode:   720 Score: -182.0  Avg.Score: -138.09, eps-greedy: 0.010 Time: 00:04:58\n",
      "Episode:   730 Score: -80.0  Avg.Score: -134.21, eps-greedy: 0.010 Time: 00:05:00\n",
      "Episode:   740 Score: -71.0  Avg.Score: -130.59, eps-greedy: 0.010 Time: 00:05:02\n",
      "Episode:   750 Score: -77.0  Avg.Score: -125.77, eps-greedy: 0.010 Time: 00:05:04\n",
      "Episode:   760 Score: -164.0  Avg.Score: -120.21, eps-greedy: 0.010 Time: 00:05:06\n",
      "Episode:   770 Score: -72.0  Avg.Score: -116.59, eps-greedy: 0.010 Time: 00:05:09\n",
      "Episode:   780 Score: -72.0  Avg.Score: -112.72, eps-greedy: 0.010 Time: 00:05:11\n",
      "\n",
      " MountainCart solved in 784 episodes!\tAverage Score: -109.48\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    scores = deque(maxlen=100)\n",
    "    temp_scores = []\n",
    "    avg_scores = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i_episode in range(n_episodes):\n",
    "        eps = epsilon_annealing(i_episode, max_eps_episode, min_eps)\n",
    "        score = play_episode(env, agent, eps)\n",
    "        \n",
    "        scores.append(score)\n",
    "        temp_scores.append(score)\n",
    "        \n",
    "        avg_score = np.mean(scores)\n",
    "        avg_scores.append(avg_score)\n",
    "        \n",
    "        dt = (int)(time.time() - start_time)\n",
    "        \n",
    "        if i_episode % 10 == 0 and i_episode > 0:\n",
    "            print('Episode: {:5} Score: {:5}  Avg.Score: {:.2f}, eps-greedy: {:5.3f} Time: {:02}:{:02}:{:02}'.\\\n",
    "                    format(i_episode, score, avg_score, eps, dt//3600, dt%3600//60, dt%60))\n",
    "            \n",
    "        if len(scores) == 100 and  avg_score >= threshold: \n",
    "                print('\\n MountainCart solved in {:d} episodes!\\tAverage Score: {:.2f}'. \\\n",
    "                    format(i_episode, np.mean(scores)))\n",
    "                break\n",
    "                \n",
    "        if i_episode % TARGET_UPDATE == 0:\n",
    "            agent.target_q.load_state_dict(agent.local_q.state_dict()) \n",
    "                \n",
    "    return temp_scores, avg_scores\n",
    "\n",
    "scores, avg_scores = train()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69da9b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.target_q,\"MountainCarModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db2ce4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.target_q = torch.load(\"MountainCarModel\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e34b00db",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = play_episode(env, agent, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94cb03d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-79.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64252ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores length:  785 , avg_scores length:  785\n"
     ]
    }
   ],
   "source": [
    "print('scores length: ', len(scores), ', avg_scores length: ', len(avg_scores))\n",
    "plt.plot(np.arange(1, len(avg_scores)+1), avg_scores, label=\"Avg on 100 episodes\")\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episodes #')\n",
    "plt.title('DQN Convergence (Avg Score: -109.99 | Episodes: 1539) with 16 hidden dimensions\\n')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ceda6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dabf5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e069800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1f29f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ac4a4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
